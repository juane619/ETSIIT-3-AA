---
title: "P3"
author: "JuanE y Adri"
date: "5 May 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# http://mlbernauer.github.io/R/20150225_coursera_ml_multiclass_classification_logistic_regression.html
# 
# https://github.com/ChicagoBoothML/MachineLearning_Fall2015/blob/master/Programming%20Scripts/Lecture06/mnist_examples.R

# https://becominghuman.ai/image-data-pre-processing-for-neural-networks-498289068258
```

# 1. AJUSTE DE MODELOS LINEALES

## PROBLEMA DE CLASIFICACIÓN: base de datos "Optical Recognition of Handwritten Digits"

### 1. Problema a resolver

El problema en si trata de ajustar un modelo lineal sobre dicha base de datos para decidir si un digito es un número entre {0,1,..,9} escritos a mano, es decir, estamos ante un problema de clasificación multi clase (10 clases).
Los conjuntos se generaron dividiendo cada digito en una matriz de 8x8 y calculando un valor de escala de grises de 0 a 16 para cada celda de la matriz promediando sus pixeles. Los datos entonces tienen 64 columnas para cada uno de los valores de escala de grises y una ultima columna (65) con el digito real (será nuestro label).

Una manera de resolver este problema podria ser crear 10 clasificadores donde cado uno clasifica entre un digito y los demás y asi con todas las clases (One vs All). Otra manera podría ser utilizando Softmax, es decir, buscamos poder clasificar usando un modelo lineal un dataset multiclase. Para hacer esto tenemos a parte de la opción One vs All, la opción softmax o función exponencial normalizada, esta es una generalización de la función logística. Su función es la de "comprimir" un vector K-dimensional, z, de valores arbitrarios (clasificaciones) en un vector K-dimensional, $\sigma(z)$, de valores reales en el rango [0, 1]. La función esta dada por:

$\sigma:\mathbb{R^{K}\rightarrow}\left[0,1\right]^{K}$
$\sigma(z)_{j}=\frac{e^{zj}}{\sum_{k=1}^{K}e^{zk}}$ for $j=1, ...,K$

La salida de la función softmax puede ser utilizada para representar una distribución categórica (distribución de probabilidad sobre $K$ diferentes posibles salidas). Por tanto, sabiendo esto podemos determinar que es idónea para nuestro problema de clasificación.

Los pasos a seguir para afrontar este problema serían en general estos:

1º Cabe señalar que lo primero que habría que hacer es asegurarnos de que todos los disgitos son igual de grandes (comparandolos mediante el número de píxeles), asumimos que esto es así pues en la web de descarga del dataset no se indica ninguna anomalía de este estilo. En cualquier caso si esto sucediese nuestra función cv.glmnet en algún momento se quejaría y mostraría el error correspondiente.

2º Normalizar los datos, esto nos sirve para no tener un cuenta unas características por encima de otras en nuestros datos. Es decir, es importante pues nos garantiza que cada parámetro de entrada (píxel, en nuestro caso) tiene una distribución de datos similar. Esto hará que la convergencia sea mucho más rápida al entrenar nuestro modelo de regresión logística. Para normalizar sacaremos el máximo de cada conjunto de pixeles (mismo pixel en cada número) y el mínimo también de cada conjunto de píxeles y una vez hecho esto a cada pixel le restamos el mínimo y lo dividimos entre el maximo menos el mínimo, de esta manera obtendremos una distribución de datos entre 0 y 1, tal cual como lo queremos y además obtendremos columnas a 0 lo cual quiere decir que el mínimo y el máximo es igual y por tanto ya podemos adelantar que estas columnas son ruido para nuestro modelo.

3º Tenemos los datos en un solo canal como es la escala de grises por lo que no necesitamos reducir la dimensionalidad de estos pues ya están correctos.

Depues de esto procederiamos con la regularización como vamos a ver más adelante.

### 2. Preprocesamiento de los datos.

Se deben normalizar los datos ya que no es igual considerar los mínimos y máximos globales (dataset completo) que considerar los de cada conjunto (mismo pixel en distintos números), es decir, pudiera ser que se "envenenaran" los conjuntos.
Esta función nos normalizará los datos para así conseguir comprimir los datos en un rango [0, 1]. 

```{r Normalization}
normalized<-function(data) {
  x<-data[!is.na(data)]
  max=max(x)
  min=min(x)
  if(max==min){
      x=0.0
    }
  else
    x<-(x - min(x)) / (max(x) - min(x))
  data[!is.na(data)]<-x
  return(data)
}

```

### 3. Selección de clases de funciones a usar.

Utilizaremos Regresión Logística ya que dicho modelo es muy usado cuando la respuesta es categórica (exactamente nuestro problema). Como nuestro modelo tiene 10 posibles salidas, nosotros usaremos el tipo de modelo Multinomial que es una generalizacion del metodo de regresion logistica para problemas multiclase (función softmax explicada anteriormente).

El modelo predice las probabilidades de los diferentes resultados posibles de una distribucion categorica como variable independiente, dado un conjunto de variables independientes.

Para este tipo de modelo la variable de salida K tiene 10 clases: G={0,1,...,9}.

Una vez conocido nuestro problema y preprocesado los datos, nuestro modelo es el siguiente:

$\mbox{Pr}(G=k|X=x)=\frac{e^{\beta_{0k}+\beta_k^Tx}}{\sum_{\ell=1}^Ke^{\beta_{0\ell}+\beta_\ell^Tx}}$

Como vemos, utiliza la regla Softmax para clasificación multilabel (extension de la clasificacion binaria).

Tenemos la matriz Y NxK, donde $y_{i\ell} = I(g_i=\ell)$, por tanto, la funcion de verosimilitud ya penalizada con LASSO es $\ell(\{\beta_{0k},\beta_{k}\}_1^K) = -\left[\frac{1}{N} \sum_{i=1}^N \Big(\sum_{k=1}^Ky_{il} (\beta_{0k} + x_i^T \beta_k)- \log \big(\sum_{k=1}^K e^{\beta_{0k}+x_i^T \beta_k}\big)\Big)\right] +\lambda \left[ (1-\alpha)||\beta||_F^2/2 + \alpha\sum_{j=1}^p||\beta_j||_q\right]$

Como vemos en dicha funcion, el parametro $\alpha$ define si la penalizacion es LASSO, RIDGE o ambas (elastic-net), nosotros en nuestro problema solo utilizaremos LASSO, es decir, con el parámetro $\alpha=1$, es decir, nos quedamos con la parte de $\alpha\sum_{j=1}^p||\beta_j||_q\right]$(lasso) y descartamos la parte $(1-\alpha)||\beta||_F^2/2$ (ridge).

### 4. Conjuntos de training, validacion y test usados.

Como los datos ya nos vienen correctamente divididos en train y test, procedemos a su lectura y a la descomposición para tener correctamente identificadas y separadas las características y las etiquetas de cada conjunto de datos (train y test). El conjunto de validación lo explicamos en la siguiente sección.

```{r}
train = read.csv("datos/optdigits_tra.csv", header = FALSE)
names(train)[65] = "digit"
test = read.csv("datos/optdigits_tes.csv", header = FALSE)
names(test)[65] = "digit"

features_train = data.matrix(subset(train, select = -digit))
labels_train   = data.matrix(subset(train, select = digit))
features_test  = data.matrix(subset(test,  select = -digit))
labels_test    = data.matrix(subset(test,  select = digit))
```

Ahora con la función definida antes, normalizamos las características:

```{r}
features_train = apply(features_train,2,normalized)
features_test = apply(features_test,2,normalized)
```

### 5. Regularización, modelo a usar e hyperparámetros.

En concreto, usaremos (ajustaremos) un modelo de regresión LASSO (R-LASSO). Este modelo de regresión lineal selecciona las variables con coeficiente mayor de un umbral prefijado. La ventaja de este modelo es que es una técnica de **regresión lineal ya regularizada**, es decir, como los datos de entrada son pixeles, es decir, pueden llegar a ser muy redundantes, dicho modelo ya penaliza variables que no dicen nada acerca de la salida, disminuyendo el correspondiente sobreajuste que esto pudiera ocasionar y además reduciendo el error fuera de la muestra.

El hiperparámetro lambda: vamos a estimar el mejor lambda mediante validacion cruzada. El mejor lambda es aquél que penaliza minimizando el error de validacion cruzada, es decir, cvm.

El paquete **glmnet** de R nos permite aplicar todo lo antes mencionado.

El paquete **glmnet** a parte de ajustar el modelo también hace un procesamiento de los datos el cual es muy necesario para el correcto funcionamiento posterior del modelo de regresión logística. Los pasos que sigue esta libreria para preprocesar son:

**glmnet** es un set de procedimientos extremadamente eficientes para ajustar todo el procedimiento de regularización lasso (también puede hacer elastic-net pero usaremos solo lasso) para regresión lineal, modelos de regresión logística multinomial (nuestro caso), regresión de Poisson y el modelo de Cox.
Los algoritmos que contiene usan lo que se conoce como descenso cíclico coordinado, que optimiza sucesivamente la función objetivo sobre cada parámetro con otros fijos y cicla hasta que converge.

Ahora hablemos del **conjunto de validación**: usaremos la funcin cv.glmnet() para ajustar el modelo usando cross-validation. Por defecto, el metodo divide el conjunto de entrenamiento antes creado en 10 trozos no superpuestos de aproximadamente el mismo tamaño, utilizando el primero de ellos como **validacion** y el resto se usa para el ajuste.

```{r}
library(glmnet)

#Para parelelizar (demasiado tiempo secuencialmente)
library(doParallel)
registerDoParallel(cores=8)

# Elegimos el mejor lambda por CV de, por ejemplo, 10 particiones (por defecto). Si utilizaramos tantas particiones como numero de muestras que tenemos, estariamos hablando de leave-one-out CV, en este caso no es recomendable por el tamaño del dataset.

#Ajuste del modelo

cvfit=cv.glmnet(features_train, labels_train, family="multinomial", type.multinomial = "grouped", parallel = TRUE, nfolds = 5)

best_lambda=cvfit$lambda.min

#Evolucion de los grados de libertad del modelo y porcentaje de desviacion segun los distintos valores de lambda.
plot(cvfit)

plot(cvfit$glmnet.fit, "norm")
abline(h=best_lambda, col= "orange", lty=4, lwd=3)

paste ("El mejor lambda es:", best_lambda)

```

Los anteriores gráficos (uno para cada clase) muestran los valores que toma lambda por cada variable predictora. A nosotros solo nos interesan aquellas que esten por encima de un umbral definido por nosotros. La linea naranja es dicho umbral, el mejor lambda. Las lineas que superan dicho umbral son aquellas variables que son mas significativas para ajustar el modelo.
Cuando la norma L1 (ejeX) es baja, todas las variables nos dicen lo mismo (estimador nulo o aleatorio, es decir, el modelo no sabe nada realmente).

Explicación de los parámetros de la función cv.glmnet:

lambda -> Son los valores de lambda usados para ajustar
cvm -> La media de los errores de validación cruzada (cross validation measure), es un vector de tamaño length(lambda)
cvsd -> Cross validation standard error de cvm, es la estimación del error estandar de cvm
cvup -> Curva superior que básicamente es cvm+csvd
cvlo -> Curva inferior que es básicamente cvm-csvd
nzero-> Cantidad de coeficientes que nos son cero con cada lambda (VALOR IMPORTANTE)
glmnet.fit -> Un objeto ajustado para todos los datos dados
lambda.min -> Valor del lambda que hace mínimo cvm
lambda.1se -> Mayor valor de lambda tal que el error estandarizado esta dentro de 1

### 8. Métrica

Una vez ajustado el modelo, es momento de medir nuestro modelo. Como métrica hemos decidido usar **matriz de confusión** ya que nos permite mostrar de forma explicita cuando una clase es confundida con otra (falsos positivos/negativos), es decir, permite trabajar de forma separada con distintos tipos de error.

```{r}
# Predicciones
library(caret)
pred_lasso_train=predict(cvfit, newx = features_train, type= "class", s="lambda.min")

confusionMatrix(data = as.factor(pred_lasso_train), reference = as.factor(labels_train))

```
La lectura de la matriz nos muestra como el modelo ha predicho muy bien las distintas muestras de train, clasificando erroneamente muy pocos digitos respecto al tamaño del conjunto.

Como vemos, tenemos varias métricas obtenidas a través de la matriz de confusión que nos seran muy utiles para medir nuestro modelo:
-   precision: numero de predicciones correctas entre el numero total de predicciones.
-   Sensibilidad y especificidad: valores que indican la capacidad de nuestro estimador para discriminar los casos positivos de los negativos. La sensibilidad se puede decir que es la tasa de verdaderos positivos. La especificidad como la tasa de verdaderos negativos.

### 9. Estimacion del error Eout.

Veamos como se comporta nuestro modelo:

```{r}
# Predicciones
pred_lasso_train=predict(cvfit, newx = features_train, type= "class", s="lambda.min")

pred_lasso_test=predict(cvfit, newx = features_test, type= "class", s="lambda.min")

#Variables mas significativas
#cvfit$`1`>best_lambda

aciertos_train=pred_lasso_train==labels_train
sprintf("El error dentro de la muestra (Ein) es %s", (length(aciertos_train[aciertos_train==F]) / length(aciertos_train)) * 100)

aciertos_test=pred_lasso_test==labels_test
sprintf("El error fuera de la muestra (Eout) es %s", (length(aciertos_test[aciertos_test==F]) / length(aciertos_test)) * 100)

```

### 10. Calidad del modelo

Se puede afirmar que el modelo es de una calidad excepcional, ya que se utilizan librerias muy probadas y testeadas que utilizan algoritmos excelentes para llevar a cabo la tarea de clasificacion. En este caso se comporta tan bien ya que el conjunto a clasificar, aunque no es perfectamente linear-separable, haciendo breves transformaciones (internamente la libreria) consigue dejar el conjunto prácticamente linear-separable , de ahi que prediga tan bien. Otra cosa que ayuda a que el modelo se comporte tan bien es que el conjunto de datos usado no tenga prácticamente ruido, es decir, los digitos usados son de bastante calidad, alomejor si testeamos el modelo con digitos con ruido o poco visibles, el modelo no se comporta tan bien.


## PROBLEMA DE CLASIFICACIÓN: base de datos "Airfoil Self Noise"

### 1. Problema a resolver

El problema, planteado incicialmente por la NASA, trata de, dados unos ejemplos medidos realmente en un tunerl de viento, predecir el ruido (dB) producido por la interacción de un ala de avión y las turbulencias a su alrededor en torno a su perfil aerodinamico.

Las caracterísitcas medidas son las siguientes: frecuencia (Hz), ángulo de ataque (grados), profundidad del ala o chord lenght (metros), velocidad maxima libre de turbulencias (m/s) y desplazamiento lateral debido a la succion y grosor del ala (metros).

Usaremos en principio un modelo de regresión lineal normal sin usar regularización y lo compararemos con un modelo de regresion lineal usando regularizacion.

### 2. Preprocesamiento de los datos.

En este caso, no hace falta normalizar (explicar por qué, creo que estaria bien decirle que los datos estan medidos en sus respectivos rangos y todas las variables son cuantitativas, por lo tanto, no es necesario normalizar).

En probabilidad y estadística, la correlación indica la fuerza y la dirección de una relación lineal y proporcionalidad entre dos variables estadísticas. Se considera que dos variables cuantitativas están correlacionadas cuando los valores de una de ellas varían sistemáticamente con respecto a los valores homónimos de la otra: si tenemos dos variables (A y B) existe correlación entre ellas si al disminuir los valores de A lo hacen también los de B y viceversa. La correlación entre dos variables no implica, por sí misma, ninguna relación de causalidad.

Procedemos a ver la correlacion de las caracteristicas para quitar alguna de estas en caso de que algunas si lo esten, es decir, de que sean dependientes y puedan empeorar la calidad del modelo ajustado.

```{r}
# Leemos datos

datos = read.csv("datos/airfoil_self_noise.csv", header = FALSE)
names(datos)[1] = "frequency"
names(datos)[2] = "angle_attack"
names(datos)[3] = "chord"
names(datos)[4] = "stream_vel"
names(datos)[5] = "disp_thick"
names(datos)[6] = "sound_level"

plot(datos)
pairs(sound_level~., data=datos)

descrCor = cor(datos) # -> Con esto vemos la correlación de las variables con las otras y ella misma (diagonal)

summary(descrCor)

#Si establecemos un punto de corte en el que decidamos cuando dos variables son dependientes (correlacionadas) en 0.75:
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
sum(highlyCorDescr)

xyplot(disp_thick~angle_attack,datos,grid=T,type = c("p", "smooth"),, col.line = "darkorange",lwd = 2)

#Como se ve en la grafica, las variables son dependientes unas de otra, por tanto, hay que quitar una de ellas. Decidimos eliminar displacement thickness.
datos=datos[,-5]
```

### 3. Selección de clases de funciones a usar.

### 4. Conjuntos de training, validacion y test usados.

```{r}
#separamos los datos en conjuntos de train y test

train.index = createDataPartition(datos$`sound_level`, p = 0.7, list = FALSE)   # Igual que StratifiedKFolds de SKlearn, mantiene la proporción de valores (iguales)
train = datos[train.index,]
test = datos[-train.index,]

features_train = data.matrix(subset(train, select = -5))
labels_train   = data.matrix(subset(train, select = 5))
features_test  = data.matrix(subset(test,  select = -5))
labels_test    = data.matrix(subset(test,  select = 5))

```


### 5. Regularización, modelo a usar e hyperparámetros.

```{r}
ans_reg <- train(sound_level ~ ., data=datos,method="lm")

summary(ans_reg)
```


### 8. Métrica



### 9. Estimacion del error Eout.

### 10. Calidad del modelo

```{r}
# Funcion para evaluar nuestro modelo ya ajustado
eval_model <- function(model) {
        pred_train <- predict(model,newdata = features_train)
        pred_test <- predict(model,newdata = features_test)
        
        # Scatter plots of predictions on Training and Testing sets
        plot(pred_train,labels_train,xlim=c(100,150),ylim=c(100,150),col=1,
             pch=19,xlab = "Predicted Noise (dB)",ylab = "Actual Noise(dB)")
        points(pred_test,labels_test,col=2,pch=19) 
        leg <- c("Training","Testing")
        legend(100, 150, leg, col = c(1, 2),pch=c(19,19))
        
        # Scatter plots of % error on predictions on Training and Testing sets
        par(mfrow = c(2, 1))
        par(cex = 0.6)
        par(mar = c(5, 5, 3, 0), oma = c(2, 2, 2, 2))
        plot((pred_train - labels_train)* 100 /labels_train,
             ylab = "% Error of Prediction", xlab = "Index",
             ylim = c(-5,5),col=1,pch=19)
        legend(0, 4.5, "Training", col = 1,pch=19)
        plot((pred_test-labels_test)* 100 /labels_test,
             ylab = "% Error of Prediction",  xlab = "Index",
             ylim = c(-5,5),col=2,pch=19)
        legend(0, 4.5, "Testing", col = 2,pch=19)
        
        # Actual data Vs Predictions superimposed for Training and Testing Data
        plot(1:length(labels_train),labels_train,pch=21,col=1,
             main = "Training: Actual Noise Vs Predicted Noise",
             xlab = "Index",ylab = "Noise (dB)")
        points(1:length(labels_train),pred_train,pch=21,col=2)
        #leg <- c("Training","Predicted Training")
        legend(0, 140, c("Actual","Predicted"), col = c(1, 2),pch=c(21,21))
        plot(1:length(labels_test),labels_test,pch=21,col=1,
             main = "Testing: Actual Noise Vs Predicted Noise",
             xlab = "Index",ylab = "Noise (dB)")
        points(1:length(labels_test),pred_test,pch=21,col="red")
        legend(0, 140, c("Actual","Predicted"), col = c(1, 2),pch=c(21,21))
        
        ## Line graph of errors
        plot(pred_train-labels_train,type='l',ylim=c(-5,+5),
             xlab = "Index",ylab = "Actual - Predicted",main="Training")        
        plot(pred_test-labels_test,type='l',ylim=c(-5,+5),
             xlab = "Index",ylab = "Actual - Predicted",main="Testing")
                
        ISRMSE<- sqrt(mean((pred_train-labels_train)^2))
        OSRMSE<- sqrt(mean((pred_test-labels_test)^2))
        
        return(c( ISRMSE,OSRMSE))
}


reg=eval_model(ans_reg)

# Preparación de los datos

# https://www.neuraldesigner.com/learning/examples/airfoil_self_noise_prediction#DataSet
# http://intellij.my/2016/03/14/regression-on-airfoil-self-noise-dataset-using-linear-regression-approach/

# http://www.datasciencedude.com/2015/05/31/test/

# http://www.synergicpartners.com/precauciones-a-la-hora-de-normalizar-datos-en-data-science/

```




