---
title: "P3"
author: "JuanE y Adri"
date: "5 May 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. AJUSTE DE MODELOS LINEALES

## PROBLEMA DE CLASIFICACIÓN: base de datos "Optical Recognition of Handwritten Digits"

### 1. Comprender el problema a resolver

El problema en si trata de ajustar un modelo lineal sobre dicha base de datos para decidir si un dígito es un número entre {0,1,..,9} escritos a mano, es decir, estamos ante un problema de clasificación multi clase (10 clases).
Los conjuntos se generaron dividiendo cada dígito en una matriz de 8x8 y calculando un valor de escala de grises de 0 a 16 para cada celda de la matriz promediando sus píxeles. Los datos entonces tienen 64 columnas para cada uno de los valores de escala de grises y una ultima columna (65) con el dígito real.

Una manera de resolver este problema podría ser crear 10 clasificadores donde cado uno clasifica entre un dígito y los demás.

```{r}
train = read.csv("datos/optdigits_tra.csv", header = FALSE)
names(train)[65] = "digit"
test = read.csv("datos/optdigits_tes.csv", header = FALSE)
names(test)[65] = "digit"

features_train = data.matrix(subset(train, select = -digit))
labels_train   = data.matrix(subset(train, select = digit))
features_test  = data.matrix(subset(test,  select = -digit))
labels_test    = data.matrix(subset(test,  select = digit))
```


- Tenemos que encontrar la manera de descubrir que características son importantes pues el dataset básicamente son píxeles y son muy redundantes (15 16 16 -> Se podría resumir en 16)

- Con esto se da la misma prioridad a cada característica, antes deberiamos de eliminar pixeles redundantes como se ha especificado antes:
```{r Normalization}
normalized<-function(y) {
  x<-y[!is.na(y)]
  x<-(x - min(x)) / (max(x) - min(x))
  y[!is.na(y)]<-x
  return(y)
}

features_train = apply(features_train,2,normalized)
features_test = apply(features_test,2,normalized)
labels_train = apply(labels_train,2,normalized)
labels_test = apply(labels_test,2,normalized)
```

- Con esta función se puede implementar regresión logística de manera muy sencilla pero no se si podremos usarla.

- Falata poner la media de comienzo mustart

```{r Logistic Regression}
modelo_clasificacion = glm(formula = labels_train ~ features_train,
                           family = binomial,
                           data = train,
                           mustart = NULL)
```